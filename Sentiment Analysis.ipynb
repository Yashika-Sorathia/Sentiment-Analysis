{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaf4d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install wordcloud\n",
    "!pip install pyLDAvis\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdc4158",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "efdc4158",
    "outputId": "616b1596-9702-4735-9120-d07eeea8c34d"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import regex as re\n",
    "from nltk import download, FreqDist\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670777d1",
   "metadata": {
    "id": "670777d1"
   },
   "outputs": [],
   "source": [
    "stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba605872",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ba605872",
    "outputId": "c99dd3cc-feda-4627-f22e-ba324a77c3cb"
   },
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv('global_warming_tweets.csv', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4b5f20",
   "metadata": {
    "id": "bb4b5f20"
   },
   "outputs": [],
   "source": [
    "tweets_df = tweets_df.loc[:, ~tweets_df.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a6b269",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56a6b269",
    "outputId": "0e9a0bb6-9b65-4454-b78f-75498b4cbd47"
   },
   "outputs": [],
   "source": [
    "tweets_df = tweets_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7293d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1933a1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove emojis\n",
    "\n",
    "def remove_emoji(tweet):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c30c601",
   "metadata": {
    "id": "4c30c601"
   },
   "outputs": [],
   "source": [
    "# remove punctuations\n",
    "\n",
    "def clean_tweets(tweet):\n",
    "  # \n",
    "  # lowercase the text\n",
    "  # tweet = tweet.lower()\n",
    "  # print('tweets:::', type(tweet)) \n",
    "  # remove mentions\n",
    "    tweet = re.sub('@[\\w]*','',tweet) \n",
    "\n",
    "    # remove hashtags\n",
    "    tweet = re.sub(\"#[A-Za-z0-9_]+\",\"\", tweet)\n",
    "\n",
    "    # remove urls\n",
    "    tweet = re.sub(r'https?:\\/\\/.*\\/\\w*', '', tweet)\n",
    "\n",
    "    # remove punctions\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
    "\n",
    "    # remove numbers\n",
    "    tweet = re.sub(r'\\d+', '', tweet)\n",
    "\n",
    "    # remove front space\n",
    "    tweet = tweet.lstrip(' ')\n",
    "    \n",
    "    tweet = remove_emoji(tweet)\n",
    "\n",
    "    return tweet\n",
    "\n",
    "tweets_df['content'] = tweets_df['content'].apply(lambda tweet: clean_tweets(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3880e7be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3880e7be",
    "outputId": "b4e2082e-3021-4de4-dff5-d4d477f802eb"
   },
   "outputs": [],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qTDCKfu8ql_9",
   "metadata": {
    "id": "qTDCKfu8ql_9"
   },
   "outputs": [],
   "source": [
    "# Generate Sentiments of the Tweets\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "tweets_df['Positive Score'] = tweets_df['content'].apply(lambda tweet: sid.polarity_scores(tweet)['pos'])\n",
    "tweets_df['Neutral Score'] = tweets_df['content'].apply(lambda tweet: sid.polarity_scores(tweet)['neu'])\n",
    "tweets_df['Negative Score'] = tweets_df['content'].apply(lambda tweet: sid.polarity_scores(tweet)['neg'])\n",
    "tweets_df['Polarity'] = tweets_df['content'].apply(lambda tweet: sid.polarity_scores(tweet)['compound'])\n",
    "\n",
    "tweets_df['Sentiment'] = ''\n",
    "\n",
    "tweets_df.loc[tweets_df['Polarity'] > 0,'Sentiment']='Positive'\n",
    "tweets_df.loc[tweets_df['Polarity'] == 0,'Sentiment']='Neutral'\n",
    "tweets_df.loc[tweets_df['Polarity'] <0,'Sentiment']='Negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LEfXH7aKxcCa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LEfXH7aKxcCa",
    "outputId": "215fd958-d558-4a10-cb8b-0296ecbb9819"
   },
   "outputs": [],
   "source": [
    "tweets_df[tweets_df['Sentiment'] == 'Positive'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Eqv7z8oFxdn0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eqv7z8oFxdn0",
    "outputId": "18c102ce-ee7e-4c9b-d18c-f54395f2160d"
   },
   "outputs": [],
   "source": [
    "tweets_df[tweets_df['Sentiment'] == 'Neutral'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1PZIMLtVyKBg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PZIMLtVyKBg",
    "outputId": "b6856fc7-b6f4-4b07-f9fa-20ec71eed664"
   },
   "outputs": [],
   "source": [
    "tweets_df[tweets_df['Sentiment'] == 'Negative'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d514c238",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['date'] = pd.to_datetime(tweets_df['date']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d62a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "tweets_list = []\n",
    "years = [2015, 2016, 2017, 2018, 2019, 2020]\n",
    "\n",
    "for year in years:\n",
    "    tweets_list.append(tweets_df[(pd.DatetimeIndex(tweets_df['date']).year == year)].shape[0])\n",
    "\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('Tweets')\n",
    "plt.title('Increase in #climatechange and #globalwarming Tweets')\n",
    "\n",
    "plt.plot(years,tweets_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0edbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = []\n",
    "negative_tweets = []\n",
    "neutral_tweets = []\n",
    "\n",
    "\n",
    "\n",
    "for year in years:\n",
    "    positive_tweets.append(tweets_df[(pd.DatetimeIndex(tweets_df['date']).year == year) & (tweets_df['Sentiment'] == 'Positive')].shape[0])\n",
    "    negative_tweets.append(tweets_df[(pd.DatetimeIndex(tweets_df['date']).year == year) & (tweets_df['Sentiment'] == 'Negative')].shape[0])\n",
    "    neutral_tweets.append(tweets_df[(pd.DatetimeIndex(tweets_df['date']).year == year) & (tweets_df['Sentiment'] == 'Neutral')].shape[0])\n",
    "\n",
    "\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('Tweets')\n",
    "plt.title('Increase in Increase in #climatechange and #globalwarming Tweets')\n",
    "plt.plot(years, positive_tweets, label='Positive Tweets')\n",
    "plt.plot(years, negative_tweets, label='Negative Tweets')\n",
    "plt.plot(years, neutral_tweets, label='neutral Tweets')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a643836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pie chart of sentiments \n",
    "pie_labels = ['Positive', 'Neutral', 'Negative']\n",
    "pie_values = [tweets_df[tweets_df['Sentiment'] == 'Positive'].shape[0], tweets_df[tweets_df['Sentiment'] == 'Neutral'].shape[0], tweets_df[tweets_df['Sentiment'] == 'Negative'].shape[0]]\n",
    "\n",
    "plt.pie(pie_values, labels=pie_labels, radius=1.5, autopct='%0.2f%%', shadow=True, colors=['#ff9999','#66b3ff','#99ff99'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95e9f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation of tweets and remove stop words\n",
    "stop_words.extend(['rt', 'amp'])\n",
    "def tokenize_tweet(tweet):\n",
    "\n",
    "    # print('clean_tweet::',clean_tweet)\n",
    "    token_list = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles= True).tokenize(tweet)\n",
    "    clean_tweet = [w for w in token_list if not w.lower() in stop_words]\n",
    "    return clean_tweet\n",
    "tweets_df['Tokenised'] = tweets_df['content'].apply(lambda text: tokenize_tweet(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3194626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_token_cloud(tweet_list):\n",
    "    comment_words = ''\n",
    "    for tokens in tweet_list:\n",
    "\n",
    "    # Converts each token into lowercase\n",
    "        for i in range(len(tokens)):\n",
    "            tokens[i] = tokens[i].lower()\n",
    "\n",
    "        comment_words += \" \".join(tokens)+\" \"\n",
    "\n",
    "    return comment_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86690ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Word Cloud for all the tweets\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "stopwords.update(['rt']) \n",
    "wordcloud = WordCloud(width = 1600, height = 800,\n",
    "                background_color ='black',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 15).generate(tweet_token_cloud(tweets_df['Tokenised']))\n",
    " \n",
    "# plot the WordCloud image                      \n",
    "plt.figure(figsize = (16, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2feea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(tweets_df['Tokenised'])\n",
    "\n",
    "# Creating Term Document Frequency \n",
    "corpus = [id2word.doc2bow(text) for text in tweets_df['Tokenised']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241a9441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA mode training\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# Set Number of Topics\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109868b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle \n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "gensimvis.prepare(lda_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed0758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic modeling on negative tweets\n",
    "id2word_negative = corpora.Dictionary(tweets_df[tweets_df['Sentiment'] == 'Negative']['Tokenised'])\n",
    "\n",
    "# Creating Term Document Frequency \n",
    "corpus_negative = [id2word_negative.doc2bow(text) for text in tweets_df[tweets_df['Sentiment'] == 'Negative']['Tokenised']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88be1de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "def get_doc_topic_dist(model, corpus, kwords=False): \n",
    "    '''\n",
    "    LDA transformation, for each doc only returns topics with non-zero weight\n",
    "    This function makes a matrix transformation of docs in the topic space.\n",
    "    \n",
    "    model: the LDA model\n",
    "    corpus: the documents\n",
    "    kwords: if True adds and returns the keys\n",
    "    '''\n",
    "    top_dist =[]\n",
    "    keys = []\n",
    "    for d in corpus:\n",
    "        tmp = {i:0 for i in range(6)}\n",
    "        tmp.update(dict(model[d]))\n",
    "        vals = list(OrderedDict(tmp).values())\n",
    "        top_dist += [np.asarray(vals)]\n",
    "        if kwords:\n",
    "            keys += [np.asarray(vals).argmax()]\n",
    "\n",
    "    return np.asarray(top_dist), keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c436724",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_dist, lda_keys= get_doc_topic_dist(lda_model, corpus, True)\n",
    "tweets_df['Topic'] = pd.DataFrame(lda_keys)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Data Preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
